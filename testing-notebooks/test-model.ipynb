{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f127854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "from utils.visualizer import SoccerVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb169c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemmapShard:\n",
    "    def __init__(self, root_dir: str, x_name: str, t_name: str, n: int, C: int, H: int, W: int):\n",
    "        self.root_dir = root_dir\n",
    "        self.n = int(n)\n",
    "        self.C, self.H, self.W = int(C), int(H), int(W)\n",
    "\n",
    "        x_path = os.path.join(root_dir, x_name)\n",
    "        t_path = os.path.join(root_dir, t_name)\n",
    "\n",
    "        self.X = np.memmap(x_path, mode=\"r\", dtype=np.float16, shape=(self.n, self.C, self.H, self.W))\n",
    "        self.T = np.memmap(t_path, mode=\"r\", dtype=np.float32, shape=(self.n, 3))\n",
    "\n",
    "\n",
    "class MemmapManifest:\n",
    "    \"\"\"\n",
    "    Provides:\n",
    "      - open shard by id (LRU)\n",
    "      - map global index -> (shard_id, local_i)\n",
    "      - load one sample (x_chw, dst_xy, y)\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir: str, cache_size: int = 2):\n",
    "        self.root_dir = root_dir\n",
    "        self.cache_size = int(cache_size)\n",
    "\n",
    "        man_path = os.path.join(root_dir, \"manifest.json\")\n",
    "        with open(man_path, \"r\") as f:\n",
    "            man = json.load(f)\n",
    "\n",
    "        assert man.get(\"format\") == \"memmap_v1\", f\"Expected memmap_v1 manifest, got {man.get('format')}\"\n",
    "        self.C = int(man[\"C\"])\n",
    "        self.H = int(man[\"H\"])\n",
    "        self.W = int(man[\"W\"])\n",
    "        self.channels = man.get(\"channels\", [])\n",
    "        self.shards = man[\"shards\"]\n",
    "\n",
    "        # prefix sums: starts[k] is global start index of shard k\n",
    "        self.starts = []\n",
    "        cur = 0\n",
    "        for s in self.shards:\n",
    "            self.starts.append(cur)\n",
    "            cur += int(s[\"n\"])\n",
    "        self.total = cur\n",
    "\n",
    "        self._cache = OrderedDict()  # shard_id -> MemmapShard\n",
    "\n",
    "    def _open_shard(self, shard_id: int) -> MemmapShard:\n",
    "        shard_id = int(shard_id)\n",
    "        if shard_id in self._cache:\n",
    "            self._cache.move_to_end(shard_id)\n",
    "            return self._cache[shard_id]\n",
    "\n",
    "        s = self.shards[shard_id]\n",
    "        mm = MemmapShard(\n",
    "            root_dir=self.root_dir,\n",
    "            x_name=s[\"x_path\"],\n",
    "            t_name=s[\"t_path\"],\n",
    "            n=int(s[\"n\"]),\n",
    "            C=self.C, H=self.H, W=self.W,\n",
    "        )\n",
    "        self._cache[shard_id] = mm\n",
    "        if len(self._cache) > self.cache_size:\n",
    "            self._cache.popitem(last=False)\n",
    "        return mm\n",
    "\n",
    "    def locate(self, k: int):\n",
    "        k = int(k)\n",
    "        if k < 0 or k >= self.total:\n",
    "            raise IndexError(f\"Global index {k} out of range (N={self.total}).\")\n",
    "\n",
    "        lo, hi = 0, len(self.starts) - 1\n",
    "        while lo <= hi:\n",
    "            mid = (lo + hi) // 2\n",
    "            start = self.starts[mid]\n",
    "            end = self.starts[mid + 1] if mid + 1 < len(self.starts) else self.total\n",
    "            if start <= k < end:\n",
    "                return mid, k - start\n",
    "            if k < start:\n",
    "                hi = mid - 1\n",
    "            else:\n",
    "                lo = mid + 1\n",
    "        raise RuntimeError(\"locate() failed unexpectedly\")\n",
    "\n",
    "    def load_by_shard_local(self, shard_id: int, local_i: int, swap_xy: bool = False):\n",
    "        shard_id = int(shard_id)\n",
    "        local_i = int(local_i)\n",
    "\n",
    "        shard = self._open_shard(shard_id)\n",
    "        if local_i < 0 or local_i >= shard.n:\n",
    "            raise IndexError(f\"local_i {local_i} out of range for shard {shard_id} (n={shard.n}).\")\n",
    "\n",
    "        # IMPORTANT: memmap is read-only -> make a writable copy to avoid PyTorch warning\n",
    "        x_np = np.array(shard.X[local_i], copy=True)  # (C,H,W) float16\n",
    "        x = torch.from_numpy(x_np).to(torch.float32)\n",
    "\n",
    "        t = shard.T[local_i]  # (3,)\n",
    "        dst_xy = torch.tensor(t[:2], dtype=torch.long)\n",
    "        if swap_xy:\n",
    "            dst_xy = dst_xy[[1, 0]]\n",
    "        y = torch.tensor(t[2], dtype=torch.float32)\n",
    "\n",
    "        x = torch.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        y = torch.nan_to_num(y, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        return x, dst_xy, y\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Model loading (two-head)\n",
    "# ---------------------------\n",
    "\n",
    "def build_model(model_name: str, in_channels: int):\n",
    "    if model_name == \"better2head\":\n",
    "        from models.passmap import BetterSoccerMap2Head\n",
    "        return BetterSoccerMap2Head(in_channels=in_channels, base=64, blocks_per_stage=2, dropout=0.0)\n",
    "    raise ValueError(f\"Unknown model_name={model_name}\")\n",
    "\n",
    "\n",
    "def load_weights(model: torch.nn.Module, ckpt_path: str, device):\n",
    "    if not ckpt_path or not os.path.exists(ckpt_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {ckpt_path}\")\n",
    "\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    if isinstance(ckpt, dict) and \"model\" in ckpt:\n",
    "        sd = ckpt[\"model\"]\n",
    "    elif isinstance(ckpt, dict) and \"state_dict\" in ckpt:\n",
    "        sd = ckpt[\"state_dict\"]\n",
    "    else:\n",
    "        sd = ckpt\n",
    "\n",
    "    missing, unexpected = model.load_state_dict(sd, strict=False)\n",
    "    print(f\"Loaded ckpt: {ckpt_path}\")\n",
    "    if missing:\n",
    "        print(f\"[warn] missing keys: {len(missing)}\")\n",
    "    if unexpected:\n",
    "        print(f\"[warn] unexpected keys: {len(unexpected)}\")\n",
    "\n",
    "\n",
    "def compute_twohead_maps(out: dict):\n",
    "    dest = out[\"dest_logits\"]\n",
    "    succ = out[\"succ_logits\"]\n",
    "\n",
    "    if dest.dim() == 4:\n",
    "        dest = dest[:, 0]\n",
    "    if succ.dim() == 4:\n",
    "        succ = succ[:, 0]\n",
    "\n",
    "    B, H, W = dest.shape\n",
    "    dest_flat = dest.view(B, -1)\n",
    "    dest_probs = torch.softmax(dest_flat, dim=1).view(B, H, W)\n",
    "    succ_probs = torch.sigmoid(succ)\n",
    "    completion = dest_probs * succ_probs\n",
    "\n",
    "    return (\n",
    "        dest_probs[0].detach().cpu().numpy(),\n",
    "        succ_probs[0].detach().cpu().numpy(),\n",
    "        completion[0].detach().cpu().numpy(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e4ce17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "\n",
    "    ap.add_argument(\"--data_root\", type=str, required=True,\n",
    "                    help=\"Root containing train/ and val/ folders with memmap manifest.json\")\n",
    "    ap.add_argument(\"--split_set\", type=str, choices=[\"train\", \"val\"], default=\"val\")\n",
    "    ap.add_argument(\"--cache_size\", type=int, default=2)\n",
    "\n",
    "    ap.add_argument(\"--ckpt\", type=str, required=True)\n",
    "    ap.add_argument(\"--model\", type=str, default=\"better2head\")\n",
    "\n",
    "    # choose sample\n",
    "    ap.add_argument(\"--example_k\", type=int, default=0, help=\"Global index within split_set (ignored if --random or shard/local provided)\")\n",
    "    ap.add_argument(\"--random\", action=\"store_true\")\n",
    "    ap.add_argument(\"--shard_id\", type=int, default=None)\n",
    "    ap.add_argument(\"--local_i\", type=int, default=None)\n",
    "\n",
    "    ap.add_argument(\"--coords_are_centers\", action=\"store_true\")\n",
    "    ap.add_argument(\"--swap_xy\", action=\"store_true\")\n",
    "\n",
    "    ap.add_argument(\"--out_dir\", type=str, default=\"viz\")\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    os.makedirs(args.out_dir, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    split_root = os.path.join(args.data_root, args.split_set)\n",
    "    mm = MemmapManifest(split_root, cache_size=args.cache_size)\n",
    "    print(f\"[data] split={args.split_set} N={mm.total} | C,H,W={mm.C},{mm.H},{mm.W}\")\n",
    "\n",
    "    # choose sample\n",
    "    if args.shard_id is not None or args.local_i is not None:\n",
    "        if args.shard_id is None or args.local_i is None:\n",
    "            raise ValueError(\"If using --shard_id or --local_i, you must provide BOTH.\")\n",
    "        shard_id = int(args.shard_id)\n",
    "        local_i = int(args.local_i)\n",
    "        k = None\n",
    "    else:\n",
    "        if mm.total == 0:\n",
    "            raise RuntimeError(f\"No samples found in {split_root}\")\n",
    "        if args.random:\n",
    "            k = int(np.random.randint(0, mm.total))\n",
    "        else:\n",
    "            k = int(np.clip(args.example_k, 0, mm.total - 1))\n",
    "        shard_id, local_i = mm.locate(k)\n",
    "\n",
    "    x_c_hw, dst_xy, y = mm.load_by_shard_local(shard_id, local_i, swap_xy=args.swap_xy)\n",
    "\n",
    "    if k is not None:\n",
    "        print(f\"[sample] split={args.split_set} k={k} -> shard={shard_id} local={local_i}\")\n",
    "    else:\n",
    "        print(f\"[sample] split={args.split_set} shard={shard_id} local={local_i}\")\n",
    "\n",
    "    # model\n",
    "    model = build_model(args.model, in_channels=x_c_hw.shape[0]).to(device).float()\n",
    "    load_weights(model, args.ckpt, device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        X = x_c_hw.unsqueeze(0).to(device, dtype=torch.float32)\n",
    "        X = torch.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        out = model(X)\n",
    "        if not isinstance(out, dict) or (\"dest_logits\" not in out) or (\"succ_logits\" not in out):\n",
    "            raise TypeError(\"Expected output dict with keys: dest_logits, succ_logits\")\n",
    "        dest_map, succ_map, comp_map = compute_twohead_maps(out)\n",
    "\n",
    "    # coords\n",
    "    dst_x = float(dst_xy[0].item())\n",
    "    dst_y = float(dst_xy[1].item())\n",
    "    if not args.coords_are_centers:\n",
    "        dst_x += 0.5\n",
    "        dst_y += 0.5\n",
    "\n",
    "    # start location from dist_to_ball argmin\n",
    "    ball_dist = x_c_hw[0].to(torch.float32)\n",
    "    flat_idx = torch.argmin(ball_dist)\n",
    "    x_idx = (flat_idx // ball_dist.shape[1]).item()\n",
    "    y_idx = (flat_idx %  ball_dist.shape[1]).item()\n",
    "    bx, by = float(x_idx) + 0.5, float(y_idx) + 0.5\n",
    "\n",
    "    # occupancy maps (your channel indices)\n",
    "    in_pos  = (x_c_hw[3] > 0).to(torch.float32)\n",
    "    out_pos = (x_c_hw[4] > 0).to(torch.float32)\n",
    "\n",
    "    vis = SoccerVisualizer(pitch_length=mm.H, pitch_width=mm.W, layout=\"x_rows\")\n",
    "\n",
    "    def _plot(name: str, heat_np: np.ndarray, title: str):\n",
    "        heat_t = torch.as_tensor(heat_np, dtype=torch.float32)\n",
    "        fig, ax, _ = vis.plot_state(\n",
    "            in_possession=in_pos,\n",
    "            out_possession=out_pos,\n",
    "            heatmap=heat_t,\n",
    "            cmap=\"Blues\",\n",
    "            heatmap_kwargs=dict(alpha=0.9),\n",
    "            add_colorbar=True,\n",
    "        )\n",
    "        ax.scatter([bx], [by], c=\"black\", s=30, marker=\"o\", zorder=6, linewidths=0.5, label=\"Start\")\n",
    "        ax.scatter([dst_x], [dst_y], c=\"red\",   s=30, marker=\"o\", zorder=6, linewidths=0.5, label=\"End\")\n",
    "        ax.set_title(title)\n",
    "        fig.tight_layout()\n",
    "        fig.legend()\n",
    "\n",
    "        tag = f\"split{args.split_set}_sh{shard_id}_i{local_i}\"\n",
    "        if k is not None:\n",
    "            tag = f\"split{args.split_set}_k{k}_sh{shard_id}_i{local_i}\"\n",
    "        out_path = os.path.join(args.out_dir, f\"{name}_{tag}.png\")\n",
    "        fig.savefig(out_path, dpi=150)\n",
    "        print(f\"saved → {out_path}\")\n",
    "\n",
    "    ok = \"✓\" if int(y.item()) == 1 else \"✗\"\n",
    "    _plot(\"dest\", dest_map, f\"Destination P(cell)  | pass {ok}  | end=({dst_x:.1f},{dst_y:.1f})\")\n",
    "    _plot(\"succ\", succ_map, f\"Success P(complete | cell)  | pass {ok}\")\n",
    "    _plot(\"complete\", comp_map, f\"Completion surface P(cell & complete)  | pass {ok}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
